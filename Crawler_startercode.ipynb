{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q78U0HsoqWCB"
   },
   "source": [
    "# **Crawler**\n",
    "This notebook contains started code structure for creating a crawler on single machine\n",
    "\n",
    "**Author:** Noshaba Nasir\n",
    "\n",
    "**Date**: 26/3/2021\n",
    "\n",
    "**Updated by**: Feza Roheel, 17L-4005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1PAzEzXScNrz"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import queue\n",
    "from urllib.parse import urlparse\n",
    "import datetime\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from socket import *\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import urllib.request\n",
    "import urllib.robotparser as urobot\n",
    "import uuid\n",
    "import threading\n",
    "from threading import Event\n",
    "import socket\n",
    "# Add any library to be imported here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "karU0DDUcDH1"
   },
   "outputs": [],
   "source": [
    "# Crawler Parameters\n",
    "BACKQUEUES= 3\n",
    "THREADS= BACKQUEUES*3\n",
    "FRONTQUEUES= 5\n",
    "WAITTIME= 15 ; # wait 15 seconds before fetching URLS from\n",
    "# Maps urls to doc ie <url,docid>\n",
    "url_to_doc = {}\n",
    "# Maps urls to robot <domain,robot>\n",
    "domain_to_robot = {}\n",
    "# crawled links\n",
    "final_links = []\n",
    "\n",
    "# Add any other global parameters here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xi8RHEdq9_e"
   },
   "source": [
    "# **FRONTIER**\n",
    "Frontier should use the Mercator frontier design as discussed in lecture.\n",
    "\n",
    "Preferably it should be a class and should have the given functions.\n",
    "\n",
    "*prioritizer* function is a stub right now, it will return a random number between 1 to f for given URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ki43PVJDYcWN"
   },
   "outputs": [],
   "source": [
    "class frontier:\n",
    "# add the code for frontier here\n",
    "# should have functions __init__, get_URL, add_URLs, add_to_backqueue\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initalizes the crawler\n",
    "        \"\"\"\n",
    "        # Name of crawler\n",
    "        self.name = \"MyCrawler\"\n",
    "\n",
    "        # Intializing domain that back queue is processing 'domain' : 'queueno'\n",
    "        self.backqueuedomain = {}\n",
    "        \n",
    "        # Intializing the front & back queues\n",
    "        self.backqueuelist = [queue.Queue(maxsize = 0) for i in range(BACKQUEUES)]\n",
    "        self.frontqueuelist = [queue.Queue(maxsize = 0) for i in range(FRONTQUEUES)]\n",
    "\n",
    "        # Starting seeds\n",
    "        self.seeds = [ \"https://docs.oracle.com/en/\",\n",
    "                      \"https://www.oracle.com/corporate/\",\n",
    "                      \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
    "                      \"https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html\",\n",
    "                      \"https://docs.oracle.com/middleware/jet210/jet/index.html\",\n",
    "                      \"https://en.wikipedia.org/w/api.php\",\n",
    "                      \"https://en.wikipedia.org/api/\",\n",
    "                      \"https://en.wikipedia.org/wiki/Weka_(machine_learning)\",]\n",
    "\n",
    "        # Adding the seed URLs in front queues based on priority\n",
    "        self.add_URLs(self.seeds)\n",
    "\n",
    "        # Atleast 1 link in each back queue using push to backqueue and update heap\n",
    "        self.time_priorityqueue = queue.PriorityQueue()\n",
    "\n",
    "        for i in range(BACKQUEUES):\n",
    "          self.add_to_backqueue(i)\n",
    "          self.time_priorityqueue.put((datetime.datetime.now(),i))\n",
    "\n",
    "\n",
    "    def front_queue_selector(self):\n",
    "      \"\"\"\n",
    "      Returns the non - empty front queue number to give chance to come in the backqueue\n",
    "      \"\"\"\n",
    "      queue_empty = 1\n",
    "      # Make sure that Front queue is not empty\n",
    "      while queue_empty == 1:\n",
    "        # More priority to later queues\n",
    "        w = list(range(1, FRONTQUEUES+1))\n",
    "        # Give chance to Front queue domain to come in backqueue\n",
    "        selected_frontqueue = random.choices(range(0, FRONTQUEUES), weights= w )[0]\n",
    "        # print(selected_frontqueue)\n",
    "        if not self.frontqueuelist[selected_frontqueue].empty():\n",
    "          queue_empty = 0\n",
    "          return selected_frontqueue\n",
    "        \n",
    "\n",
    "    def get_URL(self):\n",
    "      \"\"\" \n",
    "      Updates heap, updates backqueue and maintains politeness\n",
    "      returns url\n",
    "      \"\"\"      \n",
    "      # Get the mintime & backqueue from priority queue\n",
    "      element = self.time_priorityqueue.get()\n",
    "      time = element[0]\n",
    "      backqueue_number = element[1]\n",
    "      # get url\n",
    "      url = self.backqueuelist[backqueue_number].get()\n",
    "\n",
    "      # Politeness\n",
    "      difference = time - datetime.datetime.now()\n",
    "      seconds = difference.total_seconds()\n",
    "      if seconds > 0:\n",
    "        sleep(seconds)\n",
    "\n",
    "      # update backqueue\n",
    "      if self.backqueuelist[backqueue_number].empty():\n",
    "        self.add_to_backqueue(backqueue_number)\n",
    "      \n",
    "      # update priority queue\n",
    "      self.time_priorityqueue.put((self.get_updatedtime(), backqueue_number))\n",
    "\n",
    "      return url\n",
    "      \n",
    "        \n",
    "    def add_URLs(self, URLs):\n",
    "      \"\"\"\n",
    "      Takes in URLs\n",
    "      It will add URLs to front queues based on priority\n",
    "      \"\"\"\n",
    "      for i in range(len(URLs)):\n",
    "        # Index is from 0 to F - 1\n",
    "        queue_number = prioritizer(URLs[i], FRONTQUEUES) - 1\n",
    "        self.frontqueuelist[queue_number].put(URLs[i])\n",
    "    \n",
    "\n",
    "    def get_domain(self, URL):\n",
    "      \"\"\"\n",
    "      Takes a URL\n",
    "      It will return domain of that url\n",
    "      \"\"\"\n",
    "      domain = urlparse(URL).netloc\n",
    "      return domain\n",
    "\n",
    "\n",
    "    def get_updatedtime(self):\n",
    "      \"\"\"\n",
    "      It will return the updated time = current time + Waittime\n",
    "      \"\"\"\n",
    "      current_time = datetime.datetime.now()\n",
    "      updated_time = current_time + datetime.timedelta(0,WAITTIME)\n",
    "      return updated_time\n",
    "\n",
    "\n",
    "\n",
    "    def add_to_backqueue(self, backqueue_number):\n",
    "      \"\"\"\n",
    "      Takes a backqueue_number \n",
    "      It will add a link from frontqueue to backqueue \n",
    "      \"\"\"\n",
    "      checkbackqueue = 1\n",
    "\n",
    "      while checkbackqueue == 1:\n",
    "        # Get a non empty front queue number\n",
    "        frontqueue_number = self.front_queue_selector()\n",
    "\n",
    "        domain = self.get_domain(self.frontqueuelist[frontqueue_number].queue[0])\n",
    "\n",
    "        # Check if domain exists in already backqueue\n",
    "        if domain in self.backqueuedomain:\n",
    "          existingqueue_no = self.backqueuedomain[domain]\n",
    "          # Add domain in back queue (no need to update domain)\n",
    "          self.backqueuelist[existingqueue_no].put(self.frontqueuelist[frontqueue_number].get())\n",
    "        \n",
    "\n",
    "        # Check if backqueue is empty\n",
    "        elif self.backqueuelist[backqueue_number].empty():\n",
    "          # Add the domain from front queue & update timer\n",
    "          # Remove the domain first\n",
    "          if domain in self.backqueuedomain:\n",
    "            self.backqueuedomain.pop(domain)\n",
    "          self.backqueuedomain[domain] = backqueue_number\n",
    "          self.backqueuelist[backqueue_number].put(self.frontqueuelist[frontqueue_number].get())\n",
    "        \n",
    "        \n",
    "        if not self.backqueuelist[backqueue_number].empty() or len(url_to_doc)>=limit_documents:\n",
    "          checkbackqueue = 0\n",
    "\n",
    "\n",
    "def prioritizer(URL,f):\n",
    "    \"\"\"\n",
    "    Take URL and returns priority from 1 to F\n",
    "    Right now it like a stub function. \n",
    "    It will return a random number from 1 to f for given inputs. \n",
    "    \"\"\"\n",
    "    return random.randint(1,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "672nQJDqfIcF"
   },
   "source": [
    "# **FILTER URLS**\n",
    "Filter the URLS that are in robots.txt files of server and the have been already processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Wi8WExfwhdWy"
   },
   "outputs": [],
   "source": [
    "\n",
    "def is_url(url):\n",
    "  \"\"\"\n",
    "  Takes in url\n",
    "  returns true if the link is a url, false otherwise\n",
    "  \"\"\"\n",
    "  try:\n",
    "    result = urlparse(url)\n",
    "    return all([result.scheme, result.netloc])\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "\n",
    "def fetch(url):\n",
    "  \"\"\"\n",
    "  fetch the content and store it from the url passed as parameter\n",
    "  rteurns the unique name of the file\n",
    "  \"\"\"\n",
    "  unique_filename = uuid.uuid4().hex\n",
    "  try:\n",
    "    if url not in url_to_doc and len(url_to_doc) < limit_documents:\n",
    "      # Extract from webserver and store in xml file\n",
    "      r = urllib.request.urlretrieve(url, unique_filename + \".xml\")\n",
    "      return (unique_filename)+\".xml\"\n",
    "  except:\n",
    "    return None\n",
    "    pass\n",
    "\n",
    "\n",
    "def parse(url):\n",
    "  \"\"\"\n",
    "  Takes a url\n",
    "  returns a list of html pages urls extracted from url and also converts them into\n",
    "  absolute urls\n",
    "  \"\"\"\n",
    "  r = requests.get(url)\n",
    "  soup = BeautifulSoup(r.content, 'html.parser',from_encoding=\"iso-8859-1\")\n",
    "  links = []\n",
    " \n",
    "  for link in soup.find_all('a',href= True):\n",
    "    link = link['href']\n",
    "    if link != \"\" and link!= None and link!= '#' and is_url(link) == True:\n",
    "      # Relative urls also converted to Absolute urls\n",
    "      link = urljoin(url, link)\n",
    "     \n",
    "      # Only html pages\n",
    "      if \"text/html\" in r.headers.get(\"content-type\", ''):\n",
    "        links.append(link)\n",
    "        \n",
    "  return links\n",
    "\n",
    "\n",
    "def urlfilter(urls):\n",
    "  \"\"\"\n",
    "  takes in urls\n",
    "  for each url checks if robot.txt already stored or not. If the url is valid then \n",
    "  appends in list of filtered urls and returns it\n",
    "  \"\"\"\n",
    "  socket.setdefaulttimeout(1) # in seconds (float)\n",
    "  urlsx = []\n",
    "  for url in urls:\n",
    "    try:\n",
    "      rp = None\n",
    "      domain = urlparse(url)\n",
    "      scheme = domain.scheme\n",
    "      domain = domain.netloc\n",
    "      if domain in domain_to_robot:\n",
    "        rp = domain_to_robot[domain]\n",
    "      else:\n",
    "        rp = urobot.RobotFileParser(scheme+\"://\"+domain+ \"/robots.txt\")\n",
    "        rp.read()\n",
    "        domain_to_robot[domain] = rp\n",
    "\n",
    "      # Can fetch or not\n",
    "      if rp.can_fetch(\"*\", url):\n",
    "        urlsx.append(url)\n",
    "\n",
    "    except Exception as e:\n",
    "      continue\n",
    "  return urlsx\n",
    "\n",
    "\n",
    "def duplicateUrlElimination(urls):\n",
    "  \"\"\"\n",
    "  Takes in urls\n",
    "  returns urls that are not already processed and no duplicates\n",
    "  \"\"\"\n",
    "  processed_links = []\n",
    "  for url in urls:\n",
    "    if not url in final_links:\n",
    "      processed_links.append(url)\n",
    "  \n",
    "  return processed_links\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU0viGnCgGm9"
   },
   "source": [
    "# **RUN CRAWLER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nS1ne6rWgfN3"
   },
   "outputs": [],
   "source": [
    "# intialize every thing\n",
    "threads = []\n",
    "threadID = 1\n",
    "limit_documents = 100\n",
    "my_Lock = threading.RLock()\n",
    "my_frontier = frontier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UnnfsgTpgZpS"
   },
   "outputs": [],
   "source": [
    "# Theard task\n",
    "# define individual crawler thread's function here as studies in class\n",
    "class crawler_thread_task(threading.Thread):\n",
    "   def __init__(self, threadID, name, res,my_frontier):\n",
    "      threading.Thread.__init__(self)\n",
    "      self.threadID = threadID\n",
    "      self.name = name\n",
    "      self.res = res\n",
    "      self.my_frontier = my_frontier\n",
    "      self._has_lock = False\n",
    "\n",
    "   def run(self):\n",
    "      print (\"Starting Thread \" + self.name)\n",
    "      process_data(self.threadID, self.res, self.my_frontier)\n",
    "      print (\"Exiting Thread\" + self.name)\n",
    "      \n",
    "\n",
    "def process_data(threadId, res, my_frontier):\n",
    "  \"\"\"\n",
    "  gets url, fetches the content[polite too],extract links, filters links,\n",
    "  eliminates duplicate urls and add them in frontier. Also uses lock for\n",
    "  shared variables\n",
    "  \"\"\"\n",
    "  while True:\n",
    "    \n",
    "    try:\n",
    "      # get URL (politness also maintained in get_URL)\n",
    "      with my_Lock:\n",
    "        process_url = my_frontier.get_URL()\n",
    "      \n",
    "      # Fetch Url \n",
    "      name_of_file = fetch(process_url)\n",
    "      if name_of_file != \"\" or name_of_file!=None:\n",
    "        # Parsing url\n",
    "        new_links = parse(process_url)\n",
    "        # filter links\n",
    "        filtered_links = urlfilter(new_links)\n",
    "        # remove duplicates\n",
    "        result_links= duplicateUrlElimination(filtered_links)\n",
    "        # add resultant links to frontier\n",
    "        my_frontier.add_URLs(result_links)\n",
    "        # check to add process link or terminate\n",
    "        if check(name_of_file, process_url) == True:\n",
    "          break\n",
    "        \n",
    "    except Exception as e:\n",
    "      pass\n",
    "\n",
    "\n",
    "def check(name_of_file, process_url):\n",
    "  res = False\n",
    "  with my_Lock:\n",
    "    # only one thread can execute code there\n",
    "    if len(url_to_doc) >= limit_documents:\n",
    "      res = True\n",
    "    else:\n",
    "      if process_url not in url_to_doc and name_of_file != None:\n",
    "        url_to_doc[process_url] = name_of_file\n",
    "        final_links.append(process_url)\n",
    "        print(\"Processed: \" + process_url)\n",
    "  return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLbEV8FCgicr",
    "outputId": "6a8d5a0b-42e4-44f8-ba43-a2a277143736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Thread 1\n",
      "Starting Thread 2\n",
      "Starting Thread 3Starting Thread 4\n",
      "\n",
      "Starting Thread 5\n",
      "Starting Thread 6\n",
      "Starting Thread 7\n",
      "Starting Thread 8Starting Thread 9\n",
      "\n",
      "Processed: https://docs.oracle.com/en/\n",
      "Processed: https://docs.oracle.com/middleware/jet210/jet/index.html\n",
      "Processed: https://go.oracle.com/subscriptions\n",
      "Processed: https://www.oracle.com/corporate/\n",
      "Processed: https://www.youtube.com/oracle/\n",
      "Processed: https://academy.oracle.com/en/oa-web-overview.html\n",
      "Processed: https://developer.oracle.com/\n",
      "Processed: https://en.wikipedia.org/wiki/Weka_(machine_learning)\n",
      "Processed: https://twitter.com/oracle\n",
      "Processed: https://www.youtube.com/about/copyright/\n",
      "Processed: https://en.wikipedia.org/api/\n",
      "Processed: https://developers.google.com/youtube\n",
      "Processed: https://foundation.wikimedia.org/wiki/Developer_app_guidelines\n",
      "Processed: https://www.oracle.com/corporate/accessibility/\n",
      "Processed: https://policies.google.com/privacy?hl=ur\n",
      "Processed: https://www.youtube.com/watch?v=kg1Z72T6Ass&list=PLb8TGxLIoJD0QQtGjs3rPXXeSpm7bfh7e&index=1\n",
      "Processed: https://docs.cloud.oracle.com/iaas/developer-tutorials/tutorials/home.htm\n",
      "Processed: http://www.cs.waikato.ac.nz/~ml/weka/book.html\n",
      "Processed: https://developer.oracle.com/virtual-events/\n",
      "Processed: https://www.oracle.com/cloud/free/?source=:ow:o:s:nav::DevoGetStarted&intcmp=:ow:o:s:nav::DevoGetStarted\n",
      "Processed: https://go.oracle.com/LP=105252?elqCampaignId=276940\n",
      "Processed: https://www.youtube.com/channel/UCdDhYMT2USoLdh4SZIsu_1g\n",
      "Processed: https://www.oracle.com/legal/copyright.html\n",
      "Processed: https://apexapps.oracle.com/pls/apex/dbpm/r/livelabs/workshop-attendee-2?p210_workshop_id=649&p210_type=1\n",
      "Processed: https://go.oracle.com/LP=29630?elqCampaignId=124071\n",
      "Processed: https://go.oracle.com/LP=28277?elqCampaignId=38358&nsl=jvm\n",
      "Processed: http://www.cs.waikato.ac.nz/~ml/publications/1999/99IHW-EF-LT-MH-GH-SJC-Tools-Java.pdf\n",
      "Processed: https://twitter.com/OracleDevs\n",
      "Processed: https://www.youtube.com/ads/\n",
      "Processed: https://en.wikipedia.org/wiki/Machine_learning\n",
      "Processed: https://waikato.github.io/weka-wiki/packages/\n",
      "Processed: https://twitter.com/OracleAcademy\n",
      "Processed: https://docs.cloud.oracle.com/en-us/iaas/Content/home.htm\n",
      "Processed: http://www.kdnuggets.com/news/2005/n13/2i.html\n",
      "Processed: https://www.youtube.com/about/press/\n",
      "Processed: https://www.cs.waikato.ac.nz/~ml/publications/1994/Holmes-ANZIIS-WEKA.pdf\n",
      "Processed: https://eu.wikipedia.org/wiki/Weka_(ikasketa_automatikoa)\n",
      "Processed: https://www.oracle.com/webapps/redirect/signon?nexturl=\n",
      "Processed: https://fr.wikipedia.org/wiki/Weka_(informatique)\n",
      "Processed: http://www.cs.waikato.ac.nz/~ml/publications/1995/Garner95-imlc95.pdf\n",
      "Processed: https://fa.wikipedia.org/wiki/%D9%88%DA%A9%D8%A7_(%DB%8C%D8%A7%D8%AF%DA%AF%DB%8C%D8%B1%DB%8C_%D9%85%D8%A7%D8%B4%DB%8C%D9%86%DB%8C)\n",
      "Processed: https://nl.wikipedia.org/wiki/Weka_(software)\n",
      "Processed: http://www.youtube.com/oracle/\n",
      "Processed: https://www.youtube.com/OracleAcademyChannel/\n",
      "Processed: https://ca.wikipedia.org/wiki/Weka_(aprenentatge_autom%C3%A0tic)\n",
      "Processed: https://www.oracle.com/legal/privacy/\n",
      "Processed: https://pt.wikipedia.org/wiki/Weka\n",
      "Processed: https://docs.oracle.com/en/learn/intro_terraform_linux/index.html#introduction\n",
      "Processed: https://ko.wikipedia.org/wiki/%EC%9B%A8%EC%B9%B4_(%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)\n",
      "Processed: https://commons.wikimedia.org/wiki/Category:Weka_(machine_learning)\n",
      "Processed: https://www.youtube.com/about/policies/\n",
      "Processed: https://www.mediawiki.org/\n",
      "Processed: https://th.wikipedia.org/wiki/%E0%B9%80%E0%B8%A7%E0%B8%81%E0%B8%B2\n",
      "Processed: https://sl.wikipedia.org/wiki/Weka_(strojno_u%C4%8Denje)\n",
      "Processed: https://en.wikipedia.org\n",
      "Processed: https://si.wikipedia.org/wiki/%E0%B7%80%E0%B7%99%E0%B6%9A%E0%B7%8F_%E0%B6%B8%E0%B7%98%E0%B6%AF%E0%B7%94%E0%B6%9A%E0%B7%8F%E0%B6%82%E0%B6%9C%E0%B6%BA\n",
      "Processed: https://docs.cloud.oracle.com/en-us/iaas/Content/API/Concepts/devtoolslanding.htm#Developer_Guide\n",
      "Processed: https://www.oracle.com/cloud/iaas/training/\n",
      "Processed: https://www.youtube.com/about/\n",
      "Processed: https://twitter.com/tos\n",
      "Processed: https://lists.wikimedia.org/mailman/listinfo/mediawiki-api\n",
      "Processed: https://help.twitter.com/using-twitter/twitter-supported-browsers\n",
      "Processed: https://twitter.com/YouTube\n",
      "Processed: https://business.twitter.com/en/help/troubleshooting/how-twitter-ads-work.html\n",
      "Processed: https://id.wikipedia.org/wiki/Weka_(pembelajaran_mesin)\n",
      "Processed: https://www.youtube.com/trends/\n",
      "Processed: https://www.youtube.com/watch?v=6Khm-Bkffdk&list=PLvlciYga5j3xSy1MEPnCFtIMuE8sbKLxV&index=1\n",
      "Processed: https://he.wikipedia.org/wiki/Weka_(%D7%9C%D7%9E%D7%99%D7%93%D7%AA_%D7%9E%D7%9B%D7%95%D7%A0%D7%94)\n",
      "Processed: https://foundation.wikimedia.org/wiki/Cookie_statement\n",
      "Processed: https://www.youtube.com/howyoutubeworks/\n",
      "Processed: https://youtube.com/creatorresearch/\n",
      "Processed: https://ja.wikipedia.org/wiki/Weka\n",
      "Processed: https://ru.wikipedia.org/wiki/Weka\n",
      "Processed: https://www.oracle.com/legal/terms.html\n",
      "Processed: https://www.youtube.com/yt/dev/\n",
      "Processed: https://zh.wikipedia.org/wiki/Weka\n",
      "Processed: https://support.twitter.com/articles/20170514\n",
      "Processed: https://uk.wikipedia.org/wiki/Weka\n",
      "Processed: https://twitter.com/privacy\n",
      "Processed: https://support.google.com/youtube/answer/2814000?hl=en&p=c_strike_basics\n",
      "Processed: https://lists.wikimedia.org/mailman/listinfo/mediawiki-api-announce\n",
      "Processed: https://www.youtube.com/jobs/\n",
      "Processed: https://artists.youtube.com/\n",
      "Processed: https://studio.youtube.com/\n",
      "Processed: https://creatoracademy.youtube.com/page/education\n",
      "Processed: https://youtube.com/csai-match/\n",
      "Processed: https://www.youtube.com/premium/\n",
      "Processed: https://support.google.com/youtube/answer/2807691?hl=en\n",
      "Processed: https://www.youtube.com/creators/\n",
      "Processed: https://tv.youtube.com/\n",
      "Processed: https://stats.wikimedia.org/#/en.wikipedia.org\n",
      "Processed: https://www.youtube.com/howyoutubeworks?utm_campaign=ytgen&utm_source=ythp&utm_medium=LeftNav&utm_content=txt&u=https%3A%2F%2Fwww.youtube.com%2Fhowyoutubeworks%3Futm_source%3Dythp%26utm_medium%3DLeftNav%26utm_campaign%3Dytgen\n",
      "Processed: https://vr.youtube.com/\n",
      "Processed: http://cran.r-project.org/web/packages/e1071/index.html\n",
      "Processed: https://www.wikidata.org/wiki/Q115494#P1324\n",
      "Processed: http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/LibSVM.html\n",
      "Processed: https://github.com/rosasaul/Algorithm-SVM\n",
      "Processed: http://github.com/tomz/libsvm-ruby-swig/tree/master\n",
      "Processed: https://www.youtubego.com/\n",
      "Processed: https://cs.wikipedia.org/wiki/Weka\n",
      "Exiting Thread2Exiting Thread1\n",
      "Exiting Thread5Exiting Thread4\n",
      "Exiting Thread3\n",
      "Exiting Thread6\n",
      "\n",
      "\n",
      "Exiting Thread7\n",
      "Exiting Thread9\n",
      "Exiting Thread8\n",
      "Exiting Main Thread\n",
      "{'https://docs.oracle.com/en/': 'e4f0f19dee0946278be4dc086d06aadc.xml', 'https://docs.oracle.com/middleware/jet210/jet/index.html': 'dfb6d15a9b744bd1828f722cf8fd5d41.xml', 'https://go.oracle.com/subscriptions': '8bf108f3ff474f89af15d8ba20a1fb30.xml', 'https://www.oracle.com/corporate/': '3d2ce3f6f1cc4f0590e3fe91b7fe9eb7.xml', 'https://www.youtube.com/oracle/': 'cd9d73d419bc4bbd971cbc3885b31d37.xml', 'https://academy.oracle.com/en/oa-web-overview.html': 'bd9e782b78c44f9c90c683de1609d40b.xml', 'https://developer.oracle.com/': 'cdbec8a953f24563a66c86120d80e41a.xml', 'https://en.wikipedia.org/wiki/Weka_(machine_learning)': '224fddbc97cf43cea868d951cdee616f.xml', 'https://twitter.com/oracle': '9c5ebeaad3f046eb955d270b2cd5fd75.xml', 'https://www.youtube.com/about/copyright/': 'a226efa5aeb9492aa8c1636c7b875c34.xml', 'https://en.wikipedia.org/api/': 'd58e0ca53a1c4dfd978ed33b629b1b5c.xml', 'https://developers.google.com/youtube': '8a7b66bf821d4e4c9869f370ffcb542b.xml', 'https://foundation.wikimedia.org/wiki/Developer_app_guidelines': 'c8b0049529334b5899859c53fff3c6c5.xml', 'https://www.oracle.com/corporate/accessibility/': 'a9f9f6676cf44f3d902f9dfbcaa34979.xml', 'https://policies.google.com/privacy?hl=ur': 'fd31b0dd196943d1a78abe18d74951c5.xml', 'https://www.youtube.com/watch?v=kg1Z72T6Ass&list=PLb8TGxLIoJD0QQtGjs3rPXXeSpm7bfh7e&index=1': '1cdd97cb06624a50b2d7aabe7dba5e34.xml', 'https://docs.cloud.oracle.com/iaas/developer-tutorials/tutorials/home.htm': '479b0fcffc674e5680bcccc72c4924ee.xml', 'http://www.cs.waikato.ac.nz/~ml/weka/book.html': 'c3143c746a4444eab6655ee168a561a9.xml', 'https://developer.oracle.com/virtual-events/': '017458bb47ff40b59fb98637c40e9f88.xml', 'https://www.oracle.com/cloud/free/?source=:ow:o:s:nav::DevoGetStarted&intcmp=:ow:o:s:nav::DevoGetStarted': '90c0a923881743f98d4259556d06e71e.xml', 'https://go.oracle.com/LP=105252?elqCampaignId=276940': '963345e908db45379aadb84f462eaf96.xml', 'https://www.youtube.com/channel/UCdDhYMT2USoLdh4SZIsu_1g': '105805735b5e4523b3d3f650ec703331.xml', 'https://www.oracle.com/legal/copyright.html': '28ef5820fc60417eab0a15d8fcdc235a.xml', 'https://apexapps.oracle.com/pls/apex/dbpm/r/livelabs/workshop-attendee-2?p210_workshop_id=649&p210_type=1': 'bf378209012d4e4ea40fad1f45623088.xml', 'https://go.oracle.com/LP=29630?elqCampaignId=124071': 'b9ac1c7b393a48c78524147ccad06ebb.xml', 'https://go.oracle.com/LP=28277?elqCampaignId=38358&nsl=jvm': '437f6c7e4c094128840aafbbfb55accf.xml', 'http://www.cs.waikato.ac.nz/~ml/publications/1999/99IHW-EF-LT-MH-GH-SJC-Tools-Java.pdf': '07cb0fa30b00458b8074230e5a0a8995.xml', 'https://twitter.com/OracleDevs': 'a0103144301d4bf787e4d68401728fc5.xml', 'https://www.youtube.com/ads/': 'c0302918ee7c400686244b1ac7454074.xml', 'https://en.wikipedia.org/wiki/Machine_learning': '948c8869eb1b4f0db89c0d849b1067f9.xml', 'https://waikato.github.io/weka-wiki/packages/': 'c8685af955214478a1cc0ee7cb2f20f0.xml', 'https://twitter.com/OracleAcademy': '497953d1a4724f349fa78fed2739684b.xml', 'https://docs.cloud.oracle.com/en-us/iaas/Content/home.htm': '39bbcec640254caba570fc304b1b1919.xml', 'http://www.kdnuggets.com/news/2005/n13/2i.html': '19c7e1286b1f4441b9d3a9fd75773374.xml', 'https://www.youtube.com/about/press/': 'c772a3b492a8445a8429c6acdc0c047a.xml', 'https://www.cs.waikato.ac.nz/~ml/publications/1994/Holmes-ANZIIS-WEKA.pdf': 'f098542ec0d54a43a97929f5f5a2d3c5.xml', 'https://eu.wikipedia.org/wiki/Weka_(ikasketa_automatikoa)': 'd9671765c1c44734a0cd53a7b04c5966.xml', 'https://www.oracle.com/webapps/redirect/signon?nexturl=': '0c676117aaa54435afaed3cb1eb96770.xml', 'https://fr.wikipedia.org/wiki/Weka_(informatique)': 'f0ba7271d958441e9d80adff3e3a2c0e.xml', 'http://www.cs.waikato.ac.nz/~ml/publications/1995/Garner95-imlc95.pdf': '85409b32194744c39c9b2521ce8e6fd1.xml', 'https://fa.wikipedia.org/wiki/%D9%88%DA%A9%D8%A7_(%DB%8C%D8%A7%D8%AF%DA%AF%DB%8C%D8%B1%DB%8C_%D9%85%D8%A7%D8%B4%DB%8C%D9%86%DB%8C)': 'a217a30621aa4f36bc727fe77660caea.xml', 'https://nl.wikipedia.org/wiki/Weka_(software)': 'dd288a4c55e0432683b2d224ae7c372d.xml', 'http://www.youtube.com/oracle/': 'acbd0317bd614a36a61c0b49fa3e65d3.xml', 'https://www.youtube.com/OracleAcademyChannel/': 'bb7c1cda15f946d9bd8030dc4406c082.xml', 'https://ca.wikipedia.org/wiki/Weka_(aprenentatge_autom%C3%A0tic)': 'ffd0a700bcff4ff9957972b1ec6f8edf.xml', 'https://www.oracle.com/legal/privacy/': 'bffab161a9414d4e9acaa918a42d34a2.xml', 'https://pt.wikipedia.org/wiki/Weka': '82a88a20f4ab4e48850d1832e473d3d7.xml', 'https://docs.oracle.com/en/learn/intro_terraform_linux/index.html#introduction': 'f518abe99d004467b728dd50b27041c8.xml', 'https://ko.wikipedia.org/wiki/%EC%9B%A8%EC%B9%B4_(%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)': 'be50c956806f488bb09c46be41afa39c.xml', 'https://commons.wikimedia.org/wiki/Category:Weka_(machine_learning)': 'c4b0c0011f7745d2b487bef7c4240a57.xml', 'https://www.youtube.com/about/policies/': '76a191488ab040a98a984f1d02cd07ee.xml', 'https://www.mediawiki.org/': '8a929490cd5b4d14808d084c0ede6832.xml', 'https://th.wikipedia.org/wiki/%E0%B9%80%E0%B8%A7%E0%B8%81%E0%B8%B2': '55376d251def4eb3a2dab794a2817f00.xml', 'https://sl.wikipedia.org/wiki/Weka_(strojno_u%C4%8Denje)': 'd953c3b3b98a4468836754679e92aaab.xml', 'https://en.wikipedia.org': 'faa65ad3581a44b79dc265dce955f328.xml', 'https://si.wikipedia.org/wiki/%E0%B7%80%E0%B7%99%E0%B6%9A%E0%B7%8F_%E0%B6%B8%E0%B7%98%E0%B6%AF%E0%B7%94%E0%B6%9A%E0%B7%8F%E0%B6%82%E0%B6%9C%E0%B6%BA': '5fb3bc4619a5496789012a648fcda60b.xml', 'https://docs.cloud.oracle.com/en-us/iaas/Content/API/Concepts/devtoolslanding.htm#Developer_Guide': '1a16c5d7598641b99020fe57e3384f9d.xml', 'https://www.oracle.com/cloud/iaas/training/': '54045f5705a843658872b78f58fc0c94.xml', 'https://www.youtube.com/about/': '617c28da5c47414785c86ff8676c891e.xml', 'https://twitter.com/tos': 'e5e55701ac19406cba5da31f78dfec8d.xml', 'https://lists.wikimedia.org/mailman/listinfo/mediawiki-api': 'd35a9894e26c47acac86f6a74e1cb7d0.xml', 'https://help.twitter.com/using-twitter/twitter-supported-browsers': '9267f23654304ef1a27585c5ee56f04f.xml', 'https://twitter.com/YouTube': '975aaca3006d413fa069737316e9339d.xml', 'https://business.twitter.com/en/help/troubleshooting/how-twitter-ads-work.html': '2987c7abe4a34743be3791bc5075eb34.xml', 'https://id.wikipedia.org/wiki/Weka_(pembelajaran_mesin)': '43b7d6e4ca5f4712a4cf95175d535438.xml', 'https://www.youtube.com/trends/': '9d5e790c734a469fa555cb7b715e86b6.xml', 'https://www.youtube.com/watch?v=6Khm-Bkffdk&list=PLvlciYga5j3xSy1MEPnCFtIMuE8sbKLxV&index=1': '986951d830824ae5a00df4a5d49d1f90.xml', 'https://he.wikipedia.org/wiki/Weka_(%D7%9C%D7%9E%D7%99%D7%93%D7%AA_%D7%9E%D7%9B%D7%95%D7%A0%D7%94)': '9c849a888f5c42feab880ee7388886e0.xml', 'https://foundation.wikimedia.org/wiki/Cookie_statement': 'd4e58087fe174b9783497b02027c8e95.xml', 'https://www.youtube.com/howyoutubeworks/': '164ae7f4dca74111943aa9467fe2d355.xml', 'https://youtube.com/creatorresearch/': 'b8b28a9771ee43a7b8f32baf5551df72.xml', 'https://ja.wikipedia.org/wiki/Weka': '828a881bd0d24e2fa7aab9a9d2c0fe9f.xml', 'https://ru.wikipedia.org/wiki/Weka': 'a4cdb0c985cf4858a2f00b4f7d401cf1.xml', 'https://www.oracle.com/legal/terms.html': '1bd2c2f5fc52475d8ca1fc0f19391bd9.xml', 'https://www.youtube.com/yt/dev/': '4e57f3ca116e49edb8da59a5182950e7.xml', 'https://zh.wikipedia.org/wiki/Weka': 'c68f0f0c7207440c8ed8d0a1841e76ff.xml', 'https://support.twitter.com/articles/20170514': '938375c04bf944edbded00d152d94433.xml', 'https://uk.wikipedia.org/wiki/Weka': '6801f15160c149f884fa6afe190310f3.xml', 'https://twitter.com/privacy': '42da9d4909ce4baa8fc4de4ae5301be6.xml', 'https://support.google.com/youtube/answer/2814000?hl=en&p=c_strike_basics': '9325edd95f804c92bafcfdeaee1abd18.xml', 'https://lists.wikimedia.org/mailman/listinfo/mediawiki-api-announce': '0662c94e00114c0cb2d7489232d5a58a.xml', 'https://www.youtube.com/jobs/': 'b1ca5336a0a341e9afccd2117aec1bf5.xml', 'https://artists.youtube.com/': '47ee239772dc4451b639d5a794dc9d3b.xml', 'https://studio.youtube.com/': '3f870d258f2e4e179009ccb32da3cee2.xml', 'https://creatoracademy.youtube.com/page/education': '64be46d08fe040fda2f049ff9c14371a.xml', 'https://youtube.com/csai-match/': '913e4490bbb840129ec14d5b4a711429.xml', 'https://www.youtube.com/premium/': '5b9e420a5ed84685b42e200c2980a9cb.xml', 'https://support.google.com/youtube/answer/2807691?hl=en': '489ece79d0e04a5ea896ae443f1b97fa.xml', 'https://www.youtube.com/creators/': 'f3e82426866343b1b667f923e9f70f68.xml', 'https://tv.youtube.com/': '7081e0f2b4ef4c8abdf112bb7790e446.xml', 'https://stats.wikimedia.org/#/en.wikipedia.org': '59006dcc4bbb4a19b340b4be8b4d5528.xml', 'https://www.youtube.com/howyoutubeworks?utm_campaign=ytgen&utm_source=ythp&utm_medium=LeftNav&utm_content=txt&u=https%3A%2F%2Fwww.youtube.com%2Fhowyoutubeworks%3Futm_source%3Dythp%26utm_medium%3DLeftNav%26utm_campaign%3Dytgen': 'fd499d207f6e489aa00b9ebfbb3aed41.xml', 'https://vr.youtube.com/': '1486c3273be3402297541e81bd350f5c.xml', 'http://cran.r-project.org/web/packages/e1071/index.html': 'bfef0af767c6455c98927af9fb1d1004.xml', 'https://www.wikidata.org/wiki/Q115494#P1324': '80d81cdf9e61419ea81af2b93d8a928a.xml', 'http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/LibSVM.html': '2fb7fa09175b4f29a933f40fd1d7b6a2.xml', 'https://github.com/rosasaul/Algorithm-SVM': 'ea8dd8b139c84118ade185ea2bf8cbaf.xml', 'http://github.com/tomz/libsvm-ruby-swig/tree/master': '7789f6fb698043bb8bf0bb850aa2c953.xml', 'https://www.youtubego.com/': 'e0e3d3aa1e534828969a75b6928d4d24.xml', 'https://cs.wikipedia.org/wiki/Weka': 'c2baa455c875408b857a0f05789cc288.xml'}\n"
     ]
    }
   ],
   "source": [
    "# start the threads\n",
    "try:\n",
    "  for i in range(THREADS):\n",
    "    thread = crawler_thread_task(threadID, threadID , final_links, my_frontier= my_frontier)\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    threadID += 1\n",
    "\n",
    "  # Wait for all threads to complete\n",
    "  for t in threads:\n",
    "    if t.is_alive():\n",
    "      t.join()\n",
    "\n",
    "  print (\"Exiting Main Thread\")\n",
    "\n",
    "  print(url_to_doc)\n",
    "\n",
    "  # Stores the mapping of the files\n",
    "  doc = \"urlTodocid.txt\"\n",
    "  # url to docid file save\n",
    "  for url in url_to_doc:\n",
    "    with open(doc, \"w\") as f:\n",
    "      for url in url_to_doc:\n",
    "        f.write(url + \" , \" + url_to_doc[url] + \"\\n\")\n",
    "      f.close()\n",
    "\n",
    "except Exception as e:\n",
    "  pass\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Crawler_startercode.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
